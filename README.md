# About this course
Reinforcement Learning 強化學習原理

2024 Spring

In this course:
- Learn how to model tasks as RL problems.
- Understand RL from a theoretical viewpoint
- Learn how to systematically solve RL problems by using various RL algorithms and perform analysis of these algorithms
- Learn how to implement deep RL algorithms using software packages (e.g. Tensorflow and Pytorch) through team projects

# Progress
1. Markov Decision Process (MDP)
2. Value Iteration (VI), Policy Iteration (PI), Regularized MDPs
3. Policy Optimizaiton, Policy Gradient (PG)
4. Stochastic Policy Gradient (SPG), Variance Reduction
5. Model-Free Prediction: Monte Carlo (MC), Temporal Difference (TD), Generalized Advantage Estimator (GAE)
6. Value Function Approximation (VFA), Advantage Actor Critic (A2C)
7. Deterministic Policy Gradient (DPG), Deep DPG (DDPG)
8. Natural Policy Gradient (NPG), TD3, TRPO
9. Proximal Policy Optimization (PPO), Reinforcement Learning from Human Feedback (RLHF)
10. Epsilon-Greedy MC, Q-learning
11. Deep Q-Network (DQN), Double DQN (DDQN)
12. On-Policy vs Off-Policy, DIstributional RL
13. Direct Policy Optimization (DPO), Soft Actor Critic (SAC)
14. Imitation Learning, Inverse RL
15. Model-Based RL

# Homework & Grades
HW1: MDPs, planning, and D4RL (12%)

HW2: Policy optimization and policy gradient (11%)

HW3: DPG, DDPG, TD3, TRPO, and PPO (12%)

Theory project: [**Reinforcement Learning with Human Feedback: Learning Dynamic
 Choices via Pessimism** Summary Note](https://hackmd.io/JItAOqV_TtSaxEE63rGnZA) (30%)

Term Project: **Off-Policy Deep Reinforcement Learning without Exploration** Reproduction & Experimentation (35%)

Final Score: A+
